\section{Betriebssysteme - Grundlagen}

\question{Welche zwei Hauptaufgaben hat ein Betriebssytem?}
\begin{answer}
    \begin{itemize}
    \item Abstraktion von Geräteeigenschaften
        \begin{itemize}
            \item Geräteunabh"angige Schnittstelle zu den Anwendungen (virtuelle Maschine)
            \item Geräteüberwachung und -steuerung
        \end{itemize}
    
    \item Unterstützung des Mehrbenutzerbetriebs
        \begin{itemize}
            \item Betriebsmittelverwaltung
            \item Zuteilungsstrategien
            \item Schutz
        \end{itemize}
    \end{itemize}
\end{answer}

\question{Was ist ein Prozess?}
\begin{answer}
Ein \crucial{Programm in Ausf"uhrung}.
\end{answer}

\question{Wie ist ein UNIX-Dateisystem strukturiert? Wie können Dateien darin (eindeutig) aufgefunden
werden?}
\begin{answer}
Ein UNIX-Dateisystem ist \crucial{hierachisch organisiert}. Es \crucial{hat eine Wurzel} (root), Verzeichnisse
mit jeweils Unterverzeichnissen. In Vezeichnissen können Dateien abgelegt werden.
Vollstädnige Dateinamen sind Pfadnamen mit dem Dateinamen von der Wurzel aus.
\end{answer}

\question{Was ist ein \textit{symbolic link} (symbolischer Link)?}
\begin{answer}
\paragraph*{}
Eine symbolische Verknüpfung ist eine \crucial{Verkn"upfung} in einem Dateisystem, \crucial{die auf eine andere Datei oder ein anderes Verzeichnis verweist}. Es ist also lediglich eine Referenz auf die Zieldatei bzw. das Zielverzeichnis (d.h. ein weiterer Pfad zu einer Datei). Ein Löschen oder Verschieben der eigentlichen Datei führt üblicherweise dazu, dass die Referenz „ins Leere“ weist \crucial{(dangling link)}.

\question{Was ist ein \textit{hard link}?}
\paragraph*{}
Ein \crucial{Hardlinks} ist im Grunde genommen ein \crucial{weiterer, regul"arer Name der Datei}. Wird also die Datei unter ihrem ursprünglichen Namen gelöscht, ist sie unter ihrem zweiten Namen immer noch vorhanden. Intern wird dies durch Reference-Counting erreicht. Hardlinks \crucial{k"onnen nicht auf Verzeichnisse gelegt werden} (um Loops zu vermeiden).
\end{answer}

\question{Ist das UNIX-Dateisystem wirklich ein Baum? Begründung.}
\begin{answer}
Ein UNIX-Dateisystem entspricht eher einem \crucial{gerichteten Graphen} als einem Baum. Verzeichnisse und Dateien sind zwar hirarchisch organisiert, aber Möglichkeit von \crucial{Hardlinks}, d.h. das mehrfache Vorhandensein in der Dateistruktur einer physikalisch nur einfach vorhanden Datei stört das Bild eines Baumes. In einem Baum werden die Knoten benannt, unter UNIX werden Pfadnamen benutzt.
\end{answer}

\question{Welche Zugriffsrechte kann man auf eine UNIX-Datei haben? Welche Dateiattribute steuern dies, und wie?}
\begin{answer}
Die Zugriffsrechte sind \crucial{Lesen}, \crucial{Schreiben} und \crucial{Ausf"uhren}.
Es können unterschiedliche Rechte für den \crucial{Besitzer} (user) einer Datei, die \crucial{Gruppe} (group) und den \crucial{Rest der Welt} (others) festgelegt werden.
\end{answer}

\question{Welche Vorteile bietet es, auf Terminals in UNIX wie auf Dateien zuzugreifen? Was versteht man unter Ein-/Ausgabeumlenkung?}
\begin{answer}
Da unter \textbf{UNIX} der Zugriff auf Geräte im allgemeinen (d.h. auch auf Terminals) als Zugriff
auf eine Datei geschieht, bietet dies den Vorteil der \crucial{einheitlichen Schnittstelle}. Der Benutzer
kann auf das Terminal zugreifen wie auf eine Datei auch und muss sich daher nicht mit den konkreten Geräteeigenschaften auseinandersetzen. \\
Es gibt \crucial{Standardein- und Standardausgaben von Prozessen}. Will der Benutzer andere Ein- bzw. Ausgabemöglichkeiten nutzen, kann er Ein- und Ausgaben umlenken. Beispielsweise können die Fehlermeldungen in eine Datei umgelenkt werden.
\end{answer}

\question{Welche Aufgabe hat ein Kommando-Interpreter (z.B. in UNIX die Shell)?}
\begin{answer}
Ein Kommando-Interpreter stellt dem Benutzer eine einfache (und zugleich mächtige) \crucial{Schnittstelle zum Betriebssystem} zur Verfügung.
\end{answer}

\question{Was machst Du, wenn Dir die genaue Semantik eines Unix-Kommandos entfallen ist?}
\begin{answer}

\end{answer}

\question{bla sei ein ausführbares Programm. Was ist der Unterschied zwischen dem Aufruf\\
bla und dem Aufruf bla \& in der Shell? Welche Auswirkungen hat dies, wenn bla von Standard Input liest bzw. auf Standard Output schreibt?}
\begin{answer}

\end{answer}

\question{Gegeben sei der folgende Prozeßbaum: init $\rightarrow$ bash
Wie ändert er sich nach Eingeben der folgenden Kommandofolge im Shell?
sleep 1000 \& ;
emacs \& ;
bash;
date;}
\begin{answer}

\end{answer}


\question{Nenne drei Beispiele für Informationen, die der Betriebsystemkern über einen Prozess wissen muss.}
\begin{answer}
\begin{itemize}
\item \crucial{Prozess ID}
\item \crucial{Parent-Prozess ID}
\item \crucial{Zustand des Prozesses}
\end{itemize}

\end{answer}

\question{Was ist eine Pipe?}
\begin{answer}
Eine Pipe ist die \crucial{Umlenkung} eines \crucial{unidirektionalen sequentiellen Bitstroms} von der \crucial{Standardausgabe} eines Prozesses auf die \crucial{Standardeingabe} eines Prozesses.
\end{answer}

\question{Wie macht man ein soeben editiertes Shell-File ausführbar?}
\begin{answer}
\crucial{Setzen des Ausf"uhrbar-Bits} (executable bit) (x). \\

%% Bash script
\lstset{language=bash,caption={Setzen des \textbf{executable bit} für ein Shellskript \textit{shellfile.sh}}}
\begin{lstlisting}
chmod +x shellfile.sh
\end{lstlisting}

\end{answer}

\question{In welche Bereiche (Segmente) ist der virtuelle Adressraum eines Programmes in Ausführung unter UNIX unterteilt, und welche Eigenschaften kennzeichnen sie?}
\begin{answer}
Der virtuelle Adressraum ist in \crucial{Text}, \crucial{Data} und \crucial{Stack} unterteilt. Im Text-Teil befindet siche
der eigentliche Programmcode, im Data-Teil die Daten und auf dem Stack alle temparären
Daten (z.B. Variablen).
\end{answer}

\question{Wozu wird der Stack verwendet?}
\begin{answer}
Auf einem Stack werden die \crucial{innerhalb von Funktionsaufrufen tempor"ar anfallenden Daten eines Prozesses} abgelegt. Das umfasst unter anderem die Rücksprung-Addresse, die an die Funktion übergebenen Argumente und lokale Variablen.
\end{answer}

\question{Welchem Zweck dienen Bibliotheken (Libraries)?}
\begin{answer}
Bibliotheken \crucial{stellen Funktionen zur Verf"ugung}. Durch das Vorhandensein von Bibliotheken muss ein Programmierer nicht von Grund auf alle Funktionen neu schreiben.

\paragraph{Laut Wikipedia:}
Eine Programmbibliothek bezeichnet in der Programmierung eine Sammlung von Softwaremodulen, die Lösungswege für thematisch zusammengehörende Problemstellungen anbieten. Bibliotheken sind im Unterschied zu Programmen keine eigenständig lauffähigen Einheiten, sondern Hilfsmodule, die von Programmen angefordert werden.
\end{answer}

\question{Welche Aufgabe erfüllt ein Linker?}
\begin{answer}
Ein Linker oder Binder verbindet einzelne Programmmodule zu einem ausführbaren Programm. Im Zuge dessen werden die Symbole in den Objectfiles aufgelöst. Man unterscheidet zwischen statisches und dynamisches Linken. Man unterscheidet dabei zwischen \crucial{statisches und dynamisches Linken}.

\paragraph*{Statisches Linken:}
Die Symbole schon während des Zusammenstellens der Programmdatei aufgelöst

\paragraph*{Dynamisches Linken:}
Das Auflösen der Symbole erfolgt erst bei der tatsächlichen Ausführung des Programms.
\end{answer}

\question{Wozu wird beim Assemblieren eine Symboltabelle angelegt?}
\begin{answer}
Zum Auflösen von Verweisen.
\end{answer}

\question{Welchen Vorteil hat es, Bibliotheken mit Position Independent Code zu versehen?}
\begin{answer}
Laut Wikipedia: Position-independent Code (PIC, engl. für positionsunabhängiger Code) ist Maschinencode, der ausgeführt werden kann, unabhängig davon, an welcher absoluten Adresse im Hauptspeicher er sich befindet. PIC wird üblicherweise für dynamische Bibliotheken verwendet, damit diese für jedes Programm an eine Speicherposition geladen werden können, wo sie sich nicht mit anderen Objekten dieses Programms überlappen. 
\end{answer}

\question{Durch welche Qualitätsmerkmale soll ein Betriebssystem gekennzeichnet sein? Nenne Beispiele für konkurrierende Anforderungen.}
\begin{answer}
Effizienz, Kosten, Bedienbarkeit, Zuverlässigkeit, Verfügbarkeit, Sicherheit, Wartbarkeit.
Der Kostenpunkt kann mit jeder anderen Anforderung konkurrieren. Je besser ein Aspekt
werden soll, desto mehr Zeit und Entwicklungsarbeit, sprich Geld muss in diesen Aspekt
investiert werden. Am Beispiel Windows lässt sich wunderbar erkennen, wie Bedienbarkeit
und Zuverlässigkeit im Widerspruch zueinander stehen können. Die Sicherheit eines OS's
lässt sich oft auch nicht mit einer ausgereiften Bedienbarkeit vereinbaren, oder umgekehrt,
siehe Win vs. Linux.
\end{answer}

\question{Worin unterscheidet sich der Kernel-Mode vom User-Mode (in UNIX)? Warum wird diese Unterscheidung getroffen?}
\begin{answer}
Kernel und Useradressraum sind aus Sicherheitsgründen getrennt. Im User-Mode kann
nicht auf die Hardware zugegriffen werden. Das hat auch seinen Sinn, da diese Zugriffe
kontrolliert erfolgen sollen, damit das System sicher und zuverlässig bleibt. Ein Greuel für
jeden Admin, wenn jeder Hans und Franz in seiner Konfig rumfuhrwerken dürften.
\end{answer}

\question{Was passiert etwa bei einem System-Aufruf?}
\begin{answer}
Es erfolgt ein Trap, darauf wird der Zustand des Systems gesichert, dann erfolgt der Sprung
in den Kernel-Mode, in dem der eigentliche Hardware-Zugriff erfolgt. Daraufhin gibt es
eine Rückmeldung und einen Rücksprung zum Prozess, wo dann auch der Kontext wieder
hergestellt wird.
\end{answer}

\question{Was ist ein Interrupt? Nenne Beispiele für mögliche Interrupt-Quellen. Warum werden sie unterschiedlich priorisiert? Wie wird ein Interrupt in etwa behandelt?}
\begin{answer}
Ein Interrupt ist eine externe Unterbrechung eines Prozesses. Eine Quelle kann z. Bsp. die
Hardware sein. Hardwareinterrupts haben generell eine höhere Priorität als Softwareinterrupts.
Beim Clock-Tic ist die Quelle eben die Hardware, und der Prozess hat eine sehr hohe
Priorität, da ansonsten die System-Uhr nicht mehr genau laufen würde, was sich fatal auf
andere Programme auswirken kann, die sich nach dieser Uhr zu richten haben.
Beispiele für einen Softwareinterrupt wären SIGKILL oder SIGSTOP.
\end{answer}

\question{Was ist ein Trap? Nenne Beispiele für Traps. Inwiefern unterscheiden sich Traps von Interrupts?}
\begin{answer}
Ein Trap ist ein interne Unterbrechung eines Programms in Ausführung, die vom System
kommt. Beim Trap wird in den Kernel-Mode gewechselt, da es ja ein Systemaufruf ist.
Die Regelung erfolgt durch den Trap-Handler, der bei Bedarf dann auch Signale senden
kann. Division durch 0 wäre ein solches Beispiel. Es wird ein Trap ausgelöst, der durch den
Trap-Handler gejagt wird, welcher wiederum das Signal sigfpe lossendet (SignalFloating-
PointError). Die Folge ist, wie bei vielen anderen Signalen auch, dass der Prozess terminert
oder gekillt wird.
\end{answer}

\question{Was ist ein Signal? Nenne Beispiele für mögliche Signalquellen. Wie kann ein Prozess auf ein Signal reagieren?}
\begin{answer}
Prozesse können Signale senden und empfangen. Stop, Kill, Terminate und Sleep sind
Signale. Der Traphandler ist eine Signalquelle. Ein Signal ist die Meldung eines besonderen
Zustandes von aussen. Eine Art vordefinierte Mechanismen des Systems, die adäquat
behandelt werden, aber auch umdefiniert werden können (ignore).
\end{answer}

\question{Beschreibe kurz einige Zustände, in denen sich ein (UNIX-)Prozess befinden kann.}
\begin{answer}
Laut Wikipedia:\\
Einfaches Modell:
Sleep, Run, Zombie, Ready.

Erweitertes Modell:
dead: Der Prozess wurde beendet, er belegt jedoch noch Speicherplatz.
ready: Der Prozess wartet auf Zuteilung der CPU (Zeitschlitz). Gibt es den Ready-Zustand, so befinden sich höchstens so viele Prozesse im Zustand running, wie CPUs vorhanden sind.
running: Entweder genau der Prozess, der gerade bearbeitet wird, oder alle Prozesse, die momentan Rechenarbeit verrichten können.
sleep: Der Prozess wurde auf eigenen Wunsch zurückgestellt. Er kann Signale entgegennehmen, wie z. B. Timer, oder Ergebnisse von Kindprozessen.
trace: Der Prozess wurde von außen angehalten, üblicherweise durch einen Debugger.
wait: Der Prozess wartet auf ein Ereignis, üblicherweise eine Benutzereingabe.
uninterruptible sleep: Der Prozess wartet auf ein Ereignis, üblicherweise Hardware. Tritt dieses Ereignis ein, ohne dass der anfragende Prozess es entgegennimmt, so kann das System instabil werden.
zombie: Der Prozess wurde beendet und aus dem Arbeitsspeicher gelöscht, aber noch nicht aus der Prozessliste entfernt.
\end{answer}

\question{Nenne einige Randbedingungen, auf die man beim Entwurf eines Schedulers achten sollte. Wie sollten rechen-intensive bzw. Ein-/Ausgabe-intensive Prozesse dabei behandelt werden?}
\begin{answer}
Ein Scheduler soll schlank (effizient) und fair sein, d. h. der Rechenaufwand der Prozessverwaltung
darf nicht so gross werden, dass die Effizienz darunter leidet, und jeder Prozess soll
gerechterweise auch drankommen. Auch die Zeitscheiben sollen eine angemessene Grösse
haben.
\end{answer}

\question{Wie könnte man mit Hilfe eines Round-Robin-Schedulers Prozess-Prioritäten simulieren?}
\begin{answer}
Man könnte die Anzahl der Durchläufe für die Prozesse mit einer hohen Priorität erhöhen,
so dass diese in einem Durchlauf mehrere Male dran kommen, oder man gibt den wichtigen
Prozessen grössere Zeitscheiben, um ihnen mehr CPU-Zeit zukommen zu lassen.
\end{answer}

\question{Warum bestehen die Sleep-Queue und die Run-Queue in UNIX nicht aus jeweils einer
einzigen Warteschlange? Wie sind sie stattdessen organisiert?}
\begin{answer}
Sleep-Queue und Run-Queue bestehen aus jeweils einem Array. Jede Priorität hat darin
eine eigene Liste, das Handling zwischen den Queues ist abhängig von der Priorität. Diese
Aufteilung besteht, da sonst immer alles durchsucht werden müsste.
\end{answer}

\question{Warum werden die Zustandsinformationen eines UNIX-Prozesses teilweise in der PROStruktur
und teilweise in der User-Struktur abgelegt? Nenne jeweils drei charakteristische
Beispiele für Angaben darin.}
\begin{answer}
Die in der Proc-Struktur abgelegten Zustandsinformationen sind für alle Prozesse verfügbar,
die in der User-Struktur abgelegten sind nur für den laufenden Prozess verfügbar.
Beispiele für Angaben in der Proc-Struktur:
Signale, die zur Verarbeitung anstehen
Prozesszustände
Scheduling
Beispiele für Angaben in der User-Struktur:
Zugriffsrechte
Aktuelles Verzeichnis
Geöffnete Dateien.
\end{answer}

\question{Skizziere kurz die Prozesserzeugung in UNIX.Welche Rolle spielen die Systemaufrufe fork()
und exec() dabei?}
\begin{answer}
Der Prozess Nummer Eins (INIT) wird "von Hand" beim Systemstart erzeugt.
Andere Prozesse werden bei Bedarf als Kindprozesse mit fork() (oft als Kind-Prozesse
der Shell) erzeugt. So entsteht eine Hierachie von Prozessen. Der Kindprozess erhält eine
neue Prozess-ID, ist aber zunächst eine Kopie des Vaters, d.h. der"gleiche" Adressraum
mit gleicher Aufteilung von Text, Data und Stack. Normalerweise soll das Kind ein anderes
Programm ausführen, dieses wird mit exec() gestartet, damit wird dann auch der
Adressraum ausgetauscht.
\end{answer}

\question{Wie erfährt ein UNIX-Prozess, ob ein Kindprozess terminiert ist? Wozu gibt es in UNIX den Prozesszustand SZOMB ("Zombie")?}
\begin{answer}
Das Kind meldet bei der Termination das Signal SIGCHILD an den Vater, dieser behnadelt
durch den Aufruf wait() den Kindprozess und der Kindprozess terminiert
oder der Vater ignoriert das Signal. In diesem Fall wird das Kind zum Zombie. Der Zombie
wird dann entweder später vom Vater behandelt oder (je nach UNIX-Variante) durch das
Betriebssystem. Z.B. kann der INIT-Prozess die Proc-Struktur wieder freigeben.
\end{answer}

\question{Welche Vor- und Nachteile hat der First-Fit- bzw. der Best-Fit-Algorithmus zur Speicherverwaltung?}
\begin{answer}
Wie funktioniert der Buddy-Algorithmus in etwa?
First-Fit-Algorithmus:
Der erste Block, der gross genug ist, wird verwendet indem er in zwei Teile gespalten wird
(gross genug für die Anforderung und Rest).
Das führt zu einer Ansammlung von kleinen Blöcken am Anfang der Liste, es werden immer
mehr Blöcke und sie werden im Mittel immer kleiner. Das führt zu im Mittel längerer
Suche, kleine Anforderungen können schnell erfüllt werden.
Best-Fit-Algorithmus:
Die Liste wird immer ganz durchsucht, der Block mit dem kleinsten Verschnitt wird ausgew
ählt.
Es entstehen extrem viele kleine Blöcke. Die Suche dauert sehr lange da immer die ganze
Liste durchsucht wird. Es sind lange auch grosse Blöcke verfügbar.
Buddy-Algorithmus:
Blöcke sind immer 2k gross. Bei der Freigabe eines Blockes wird er, wann immer sein
Nachbar (Buddy) frei ist, mit dem Nachbarn verschmolzen.
Das vereinfacht das splitten und zusammenfügen der Blöcke. Allerding entsteht freier Speicher
innerhalb der Blöcke.
Optimierung: mehrere Listen mit jeweils gleich grossen Blöcken, das verringert die Suchdauer.
\end{answer}

\question{Wozu bieten Systeme eine Speicherhierachie an? Welche Beobachtung über den Speicherzugriff realer Programme liegt dem zugrunde? Welche verschiedenen Arten von Speicher
werden typischerweise bereitgestellt?}
\begin{answer}
Die Systeme bieten eine Speicherhierachie an da die Geschwindigkeit sich konträr zu den
Kosten verhält.
Reale Programme greifen oft nur auf eine sehr kleine Bereich zu (Working Set).
Es werden meist Cache, Hauptspeicher und Massenspeicher bereitgestellt.
\end{answer}

\question{Warum ist es in der Regel nicht sinnvoll, den Adressraum eines Prozesses in einem Stück im Speicher abzulegen?}
\begin{answer}
Ein Prozess muss nicht zwangsläufig komplett im Speicher verfügbar sein (s.o.:Working
Set). Bei der Aufteilung in kleinere Einheiten ist auch die Nutzung kleinerer Freispeichereinheiten
möglich, heisst insgesamt eine erheblich bessere Nutzung des Haupspeichers.
\end{answer}

\question{Was versteht man unter Paging, was unter Segmentierung? Wo tritt interne Fragmentierung, wo externe Fragmentierung auf? Was versteht man darunter?}
\begin{answer}
Segmentierung:
Speicher wird in verschieden grosse Stücke unterteilt, das ermöglicht die 
exible Zuteilung
kleiner Speicherbereiche und Shared Memory.
Es sind Adressumsetzungstabellen pro Prozess notwendig.
Segmente enthalten unterschiedlich viele Pages und sind maximal 210Bytegross:
Paging:

exible Zuteilung von gleich grossen Speichereinheiten. Der Hauptspeicher wird in Kacheln (PageFrames)
fester Grösse unterteilt, die Prozessadressräume wiederum liegen in Seiten (Pages) von
gleicher Grösse. Auch dieses Verfahren ermöglicht Shared Memory, impliziert aber interne Fragmentierung.
externe Fragmentierung: wenn durch die angewendete Logik Speicher in unterschiedlich grosse
Stücke eingeteilt wird und ganze Stücke freibleiben.
interne Fragmentierung: wenn innerhalb von gleich grossen Stücken der Speicher nicht restlos
ausgenutzt wird und so Speicher ungenutzt bleibt.
\end{answer}

\question{Aus welchen Teilen besteht eine virtuelle Adresse zumeist? Wie ermittelt sich daraus die entsprechende Hauptspeicheradresse, d.h. wie läuft die Adressverwaltung in etwa ab?}
\begin{answer}
Eine virtuelle Adresse besteht zumeist aus zwei Teilen: jPAGEjADDRj. Der erste Teil wird bei
Gebrauch in der Page-Tabelle "nachgeschaut" und entsprechend ersetzt: jADDRjADDRj.
Dieser Vorgang wird üblichwerweise durch Hardwareunterstützung realisiert: die Memory Managment
Unit (MMU).
\end{answer}

\question{Wie können mehrere Prozesse mit Hilfe virtueller Adressierung auf dieselben Programmstücke (oder auch Datenbereiche) zugreifen?}
\begin{answer}
Shared Memory:
In den jeweiligen (unterschiedlichen) Page-Tabellen sind dieselben Seiten eingetragen. (Doppelter
bzw. dreifacher Eintrag ein und desselben Inhalts in die 'Inhaltsverzeichnisse').
\end{answer}

\question{Wie arbeiten die folgenden Algorithmen zur Verdrängung von Seiten aus dem Hauptspeicher in etwa?}
\begin{answer}
LRU (Least-Recently-Used)
LRU entfernt die Seite im Hauptspeicher, auf die am längsten nicht mehr zugriffen
wurde. Das Lokalitätsprinzip wird in der Regel gut erfasst. LRU besitzt eine gute
Aproximation an den optimalen Algorithmus,
aber das erfassen aller Zugriffszeiten auf die Seiten ist notwendig, d.h. bei jedem
Zugriff müssen weitere Speicherzugriffe erfolgen. LRU ist zu aufwendig ohne Spezialhardware.

FIFO (First-in-First-Out)
FIFO entfernt die älteste Seite und ist einfach zu realisieren: als verkettete Liste der
Page Frames nach Belegungsalter.
Aber: bestimmt das Working Set in der Regel nicht gut.

Second-Chance-FIFO
Funktioniert (im Prinzip) wie FIFO: als verkettete Liste nach Belegungsalter. Wenn
eine Seite (nach FIFO) zum Löschen an der Reihe ist wird überprüft ob sie seit
ihrer letzten überprüfung referenziert wurde. Wenn ja: sie wird wieder (als jüngste
Seite) in die Liste eingehängt. Wenn nein: löschen der Seite.
In diesem Algorithmus wird unterschiedliche Benutzungshäufigkeit einkalkulliert,
aber er ist sehr aufwendig, da auch eine FIFO-Liste geführt werden muss (s.o.)

NRU (Not-Recently-Used)
NRU trifft eine zufällige Auswahl aus den kürzlich nicht referenzierten Seiten (z.B.
mit zyklischer Suche nach Page Frame-Nummern). Der gewählte Modus des zurücksetzens
des Referenzbits entscheidet über die Güte".
Mögliche Optimierung:
Unterscheidung zwischen Nur-Lese-Zugriffen und Schreib-Zugriffen, wird in Dirty-
Bit (D) = Modifikationsbit (M) angegeben.
Da bei kürzlich beschriebenen Seiten der veränderte Seiteninhalt erst gerettet werden
muss, sollte wegen des Aufwandes eher eine Seite mit Nur-Lese-Zugriffen zum
überschreiben gwählt werden.

Bei Lastspitzen summiert sich die Verdrängung von beschriebenen Seiten, daher
wird NRU oft nicht verwendet.
Aging
Beim Aging altern die Seiten duch shiften eines Schieberegisters das für jede Seite
angelegt wird. Das führt zu einer guten Annäherung an LRU, aber auch zu unbertretbar
hohem Aufwand. Bei Vereinfachung des Algorithmus (z.B. von Schieberegister
mit 8 Bit auf 2 Bit) gehen die LRU-ähnlichen Vorteile verloren und Aging wird
zu komplexen Variante von Second-Chance-FIFO.
\end{answer}

\question{In welche dieser Kategorien kann man den Clock-Hand-Algorithmus einordnen?}
\begin{answer}
LRU (ohne Dirty-Bit)
\end{answer}

\question{Warum ist ein optimaler Algorithmus nicht realisierbar?}
\begin{answer}
Ein optimaler Algorithmus ist für ein 'normales' Betriebssystem nicht realisierbar, da die Prozesse
durch unterschiedliche Working Sets eine unterschiedlich grosse Anzahl von Seiten unterschiedlich
lange benötigen.
Im Gegensatz dazu ist bei einem Betriebssystem mit wenigen, speziellen Aufgaben (Embeded
Systems) ein optimaler bzw. annähernd optimaler Algorithmus möglich.
\end{answer}

\question{Was passiert, wenn die Umlaufzeit des Zeigers beim Clock-Hand-Algorithmus zu gross bzw. zu
klein gewählt wird? Wie kann ein zweiter Zeiger den Algorithmus verbessern?}
\begin{answer}
Wenn die Umlaufzeit zu gross gwählt wird ist irgendwann kein freier Speicher mehr verfügbar (die
Freispeichereserve leer).
Wird die Umlaufzeit zu klein gewählt, werden die Seiten vor ihrer nächsten Benutzung aus dem
Speicher entfernt und müssen dann erneut geladen werden, die Freispeicherreserve ist grösser als
sinnvoll.
Ein zweiter Zeiger verbessert den Clocl k-Hand-Algorithmus insbesondere bei grossen Mengen von
Speicher. Z.B. kann der erste Zeiger das Referenzbit gegebenenfalls zurücksetzen, der Zweite die
Seiten entfernen falls sie in der Zwischenzeit nicht erneut referenziert wurden.
\end{answer}

\question{Was ist Swapping?Warum wenden auch Paging-Systeme häufig dieses Verfahren an? Unter welcher Bedingung?}
\begin{answer}
Swapping wird angewandt wenn die Working Sets der aktiven Prozesse nicht vollständig in den
Speicher passen.
Dadurch entsteht eine hohe Page Frame-Rate, das so genannte Seiten-Flattern. Da das Lesen
von der Festplatte recht lange dauert, sinkt die Geschwindigkeit, in der die Instruktionen abgearbeitet
werden können, rapide.
Durch Swapping werden nicht nur Teile von Prozessen (Paging), sondern ganze Prozesse auf die
Platte ausgelagert. Längere Zeit inaktive Prozesse werden aus dem Hauptspeicher entfernt, um
dort Platz verfügbar zu machen, dadurch wird natürlich ein Austausch der Prozesse im Hauptspeicher
nach gewisser Zeit erforderlich.
\end{answer}

\question{Wie kann man die Vorteile von Paging und Segmentierung kombinieren?}
\begin{answer}
Bei einem einzigen Adressraum kann dieser beim Paging nicht logisch aufgeteilt werden. Der gesamte
Adressraum wird in gleich grosse Kacheln eingeteilt.
Deswegen wird heute der Speicher häufig in mehrere Segmente unterteilt, die jeweils eigene Pagetabellen
enthalten. Die Grösse der Segmente ist nicht statisch, sondern kann jede Gösse mit n
multipliziert mit Page Frame Grösse umfassen.
Dadurch entstehen dreiteilige Adressen:
Region mit eigener Pagetabelle, Page in der Region, Adressein Page.
Regionen folgen dabei der logischen Grösse des Adressraumes (Text, Data, Stack), Pages sind in
feste Grössen unterteilt.
\end{answer}

\question{Wozu wird bei der Speicherverwaltung häufig ein Assoziativspeicher eingesetzt?}
\begin{answer}
Assoziativspeicher wird im allgemeinen als Spezialhardware innerhalb der MMU (Memory Managment
Unit) realisiert. D.h. Assoziativspeicher == Hardware-Cache.
Er wird häufig zur Adressumsetzung genutzt. Für die gesamte Pagetabelle wäre dies sehr teuer,

deswegen wird beim Prozesswechsel neu geladen, es sind (getreu dem Lokalitätsprinzip) immer nur
einige Seiten in Gebrauch. Daher enthält der Cache jeweils nur diese Seiten. Ablauf: Adresszugriff,
Page Table Entry im Cache? Wenn ja: Adressumsetzung, wenn nein: Nachladen aus der Pagetabelle.
Es ist der parallele Zugriff auf alle im Cache befindlichen Einträge möglich. Die gesamte Pagetabelle
im Hauptspeicher vorzuhalten, wäre zu langsam, da zusätzlicher Speicherzugriff nötig
wäre.
\end{answer}

\question{Beschreibe kurz die Zugriffsoperationen open(), close(), lseek(), read() und write() auf ein UNIXFilesystem.}
\begin{answer}
Welche Rolle spielt der File Descriptor dabei?
open()
Öffnet eine Datei für die weitere Arbeit. open() werden die Parameter path (Pfadname der Datei),

ags (erlaubte Folgeoperationen wie lesen, schreiben,...) und mode (Zugriffsrechte bei einer NEU
angelegten Datei) übergeben. Der File Descriptor wird zurückgegeben.
close()
Dieser Systemaufruf gibt eine Datei wieder frei.
create()
Es wird mit den Parametern path und mode eine neue Datei erzeugt. Der File Descriptor wird
zurückgegeben.
lseek()
wird mit den Parametern File Descriptor, offset und whence aufgerufen. Die aktuelle Position im
File Descriptor wird um offset Bytes gemäss whence verschoben (d.h. 0 = vom Anfang, 1 = vom
Ende, 2 = von der aktuellen Position aus).
Zusammengefasst: lseek setzt den Zeiger, der innerhalb einer Datei die Position anzeigt.
read()
mit den Parametern File Descriptor (fd), buf und len liest von len Bytes ab der aktuellen Position
des fd in den Puffer buf, dabei wird die aktuelle Position um die gelesenen Bytes weitergeschoben.
read() liefert die Anzahl der tatsächlich gelesen Bytes.
write()
mit den Parametern fd, buf und len funktioniert analog zu read() und liefert die Anzahl der geschriebenen
Bztes zurück.
\end{answer}

\question{Wie sieht die Struktur des UNIX-V7-Dateisystems auf der Platte in etwa aus? Warum erfolgt die Verwaltung der Freispeicherliste über Indirekt-Blöcke?}
\begin{answer}
Das Dateisystem V7:
- Der Dateiinhalt ist Block-orientiert mit speziellem Block-Index organisiert.
- Der Boot-Block ist Block 0 des Root-Dateisystems und wird beim booten geladen.
- Der Superblock enthält die Verwaltungsinformationen des Dateisystems: Grösse, Verwaltung der
freien Inodes, Verwaltung der freien Blöcke (mit verketteter Liste von Blöcken mit freien Blocknummern)
- Inodes dienen zur Ablage der Verwaltungsinformationen der Dateien. Jede Datei hat einen Inode
mit:
+ eindeutigem Bezeichner (heisst: Inodenummer, diese gibt die Position an)
+ Besitzer (nid), Gruppe (gid)
+ Zeitpunkt der letzten änderung, des letzten Zugriffs, ...
+ Anzahl der Hard Links (mehrere Namen für eine Datei verweisen auf denselben Inode)
+ Anzahl von Bytes
+ Dateityp
+ Verweise auf Datenblöcke
+ Zugriffsrechte
Kleine Dateien bis zu zehn Blöcken werden direkt im Inode-Block gespeichert.
- In Indirekten Blöcken sind alle Dateien grösser als zehn Blöcke organisiert. Jeder Indirekt Block
hat auch ist auch in den Inodes mit einem Eintrag vorhanden. Mit Indirekten Blöcken sind grosse
Dateien realisierbar, mit maximal vier Zugriffen für einen Dateiblock.
Die Aufteilung in verschiedene Blöcke dient insbesondere der Sicherheit, so wird z.B. der Superblock,
ohne den der Rest des Systems nicht mehr zugreifbar ist, mehrfach an verschiedenen Stellen
der Platte gespeichert. So können die wichtigsten Informationen bei der Zerstörung des Original-
Superblockes wiederhergestellt werden. Des weiteren können durch Verschleiss kappute Blöcke als
nicht benutzbar deklariert werden. Das Gegenbeispiel zur Organisation in Blöcken ist die Organisation
als verkettete Liste, heisst bei eine Störung der verkettene Liste durch einen kapputen

Block (z.B.) sind alle folgenden Listeneinträge nicht mehr erreichbar.
\end{answer}

\question{Welche Aufgaben enthält ein Inode? Welche Angaben enthält eine Verzeichnis-Datei (Directory)?}
\begin{answer}
Aufgaben von Inodes: siehe oben. Eine Verzeichnis-Datei (Directory) ist eine Datei und hat entsprechend
einen eigenen Inode und Datenblöcke. Verzeichnisse sind eine Folge von Einträgen, die
jeweils den Dateinamen (auch Dateinamen von Unterverzeichnissen) und die jeweilige Inodenummer
enthalten.
\end{answer}

\question{Welche Aufgaben hat der Buffer Cache in UNIX?}
\begin{answer}
Der UNIX-Buffer Cache ist ein Zwischenspeicher (Puffer) für gelesene oder geschriebene Plattenbl
öcke im Kernadressraum. Er puffert Ein- und Ausgabe vom User-Adressraum entkoppelt. Der
Buffer Cache ermöglicht Mehrfachzugriff ohne weitere Plattenzugriffe.
Die Daten werden von Platte zu Cache in Blöcken transportiert, von Cache zu CPU in Bytes.
\end{answer}

\question{Was geschieht durch einen mount()-Systemaufruf in etwa?}
\begin{answer}
Unter UNIX kann eine physikalische Harddisk in mehrere logische Partitionen eingeteilt sein. Jede
dieser Patitionen enthält ihr eigenes Dateisystem, ein UNIX-System kann also aus mehreren solcher
Dateisysteme zusammengesetzt sein. Mit dem Befehl mount wird ein weiteres Dateisystem
in einen solchen Dateibaum (Wurzel: Root) eingehängt, d.h. verfügbar gemacht. Beim mounten
werden logische Geräte (z.B. CDROM, Partition, ...) und der Mount Point übergeben.
\end{answer}

\question{Welche Vorteile bietet es, Dateien mit dem UNIX-Systemaufruf mmap() in den virtuellen Adressraum eines Prozesses abzubilden?}
\begin{answer}
I/O Performance steigt durch:

\begin{itemize}
\item Lazy Loading 
\item Keine Systemaufrufe notwendig
\item Das Betriebssytem arbeitet meistens direkt auf dem Buffercache, so dass keine Kopie im Userspace Addressraum angelegt werden muss.
\end{itemize}

\end{answer}

\question{Wie ist eine Platte intern organisiert? Wie wirkt sich dies auf den Informationszugriff aus? Wie geht das UNIX Fast File System damit um?}
\begin{answer}
Eine physikalische Festplatte besteht aus sechs bis zehn internen Platten (Scheiben). Auf jede
dieser internen Platten greift jeweils von oben und von unten ein Lese-Schreibkopf zu. Jede
Oberfläche ist in Spuren unterteilt, jede Spur in Sektoren. übereinanderliegende Spuren auf den
verschiedenen Oberflächen bilden Zylinder. Da für jede Armbewegung relativ viel Zeit benötigt
wird, sollten Armbewegungen möglichst vermieden werden. D.h. zusammengehörende Daten sollten
in hintereinanderliegenden Sektoren in derselben Spur untergebracht werden, damit das Lesen
und Schreiben in Vorgang ohne weitere Armbewegung vorgenommen werden kann. Ist die Datenmenge
grösser, sollten die Spuren innerhalb des gleichen Zylinders verwendet werden.

% FIXME
UNIX Fast File System:
Das System arbeitet auf vier bzw. acht Byte grossen Datenblöcken. Die Platte ist in Zylindergruppen unterteilt. Jede Zylindergruppe hat einen eigenen Inode- und Datenbereich, Dateien und ihr Inode werden möglichst in derselben Zylindergruppe gespeichert. Ein Verzeichnis und die darin enthaltenen Dateien werden wiederum in derselben Zylindergruppe gespeichert, dies gilt aber nicht für im Verzeichnis befindliche Unterverzeichnisse.
\end{answer}

\question{Welche Vorteile bietet eine vereinheitlichte Betriebssystemschnittstelle für den  Zugriff auf Geräte? Wie sieht die in UNIX in etwa aus?}
\begin{answer}
In UNIX sind Geräte als Datein dargestellt. Dies bietet für Anwender und Programmierer den
Vorteil, dass sie einheitlich zu behandeln sind.

\paragraph*{Laut Wikipedia:}
Everything is a file (engl. ‚Alles ist eine Datei‘) beschreibt eine der definierenden Eigenschaften von Unix und seinen Abkömmlingen, dass eine große Bandbreite an Ein-/Ausgabe-Ressourcen wie Dokumente, Verzeichnisse, Festplatten, Modems, Tastaturen, Drucker und sogar Interprozess- und Netzwerkverbindungen als einfache Byteströme via Dateisystem verfügbar sind.
\end{answer}

\question{Was ist ein Gerätetreiber, was ein Geräte-Controller? Welche Aufgaben haben sie?}
\begin{answer}
Gerätetreiber:
- ist Code innerhalb des Betriebssystems zur Geräteverwaltung
- es existiert jeweils ein Treiber pro Gerätetyp, die Typen werden durch die Major Number unterschieden
- als Parameter wird die Minor Number benötigt, um die konkrete Hardware zu identifizieren /
anzusprechen. Controller:
- ist Hardware, die sich zwischen CPU und Gerät befindet
- enthält unter anderem einen Puffer für die Zwischenlagerung von Aufträgen an das Gerät
- zur Aktivierung des Controllers werden die Aufträge in Controllereigenen Registern abgelegt
Ein Treiber ist also zum Kernel gehörende Software, ein Controller ist die zu einem Gerät gehörende,
vom Treiber gesteuerte, Hardware.
\end{answer}

\question{Warum erfolgt der Zugriff auf Geräte häufig über Warteschlangen? Wozu besitzten diese in der Regel eine High Water Mark bzw. eine Low Water Mark?}
\begin{answer}
Der Zugriff über Warteschlangen erfolgt um eine Vermischung von Aufträgen und das Verlorengehen von Aufträgen zu verhindern. Desweiteren um die komplette Abarbeitung eines Auftrages zu gewährleisten und die Reihenfolge der Auftragsabarbeitung zu organisieren.
Die Warteschlangen besitzen in der Regel eine High Water Mark, um anzuzeigen, dass die Warteschlange voll ist, d.h. um zu verhindern das entweder ein Auftrag in der Schlange überschrieben wird oder ein Auftrag verlorengeht. Die Low Water Mark zeigt dagegen an, dass weitere Aufträge "nachgefüllt" werden können.
\end{answer}

\question{Worin unterscheidet sich Direct Memorz Access (DMA) von Programmed I/O?}
\begin{answer}
Beim Programmed
I/O ist der Ablauf wie unter 7.2 beschrieben, da der Treiber ein Teil des Betriebssystems ist, bleibt letztendlich die Kontrolle des Vorgangs bei der CPU. Beim Direct Memory Access (DMA) liegt nach dem Anstoss eines Auftrages der Zugriff auf den
Hauptspeicher und die daraus vorzunehmenden Kopiervorgänge in der "Verantwortung" des Controllers.
Der Controller greift nach der Abarbeitung eines Auftrages ohne Einbeziehung der CPU
auf den Hauptspeicher zu. Zu diesem Zweck muss der Auftrag selbst die entsprechenden Speicherbereiche
angeben.
\end{answer}

\question{Warum werden Terminal-Treiber in UNIX parametrisiert? Nenne typische Parameter.}
\begin{answer}
Geräte können unter Umständen in verschiedenen Modi laufen. Um den gewünschten Modus zu
erzeugen, benötigt ein Treiber die Angabe desselben mit Hilfe eines Parameters => Parametrisierung.
Ein typischer Parameter wäre bei einem Monitor z.B. die Farbtiefe.
Bei Terminals werden grundsätzlich zwei Modi unterschieden: der Raw-Modus (Canonical Mode),
in ihm werden einem Terminal Tasteneingaben unverändert und zeichenweise an des Pozess
weitergereicht. Im Cooked-Modus (Noncanonical Mode)wird die Tasteneingabe zeilenweise an den
Prozess weitergegeben. In diesem Modus ist es möglich, Tastenkombinationen abzufangen (also
erst einmal nicht an den Prozess weiterzugeben) und ihnen besondere Funktionen zuzuweisen.
Z.B. Zeileneditierfunktionen wie: BS, DEL, Strg-w,... zu verarbeiten und dann in der neuen Form
an den Prozess weiterzugeben. Weitere Beispiele hierfür sind die Flusskontrolle mit Strg-s (stoppt
Ausgabe) oder Strg-q (weiter) oder Signale wie Strg-c (stoppt den Prozess mit SIGINT).
\end{answer}
